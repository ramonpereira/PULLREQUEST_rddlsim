//////////////////////////////////////
//Buser.rddl
//
//Author:Buser
//////////////////////////////////////

domain Navigation_Problem{

	requirements = { 
		reward-deterministic 
	};

	types {
		dim: object;
	};
	
	pvariables {
	
		// Constant
		MINMAZEBOUND(dim): { non-fluent, real, default = 0.0 };
		MAXMAZEBOUND(dim): { non-fluent, real, default = 10.0 };
		MINACTIONBOUND(dim): { non-fluent, real, default = -0.5 };
		MAXACTIONBOUND(dim): { non-fluent, real, default = 0.5 };
		MINMUDBOUND(dim): { non-fluent, real, default = 3.0 };
		MAXMUDBOUND(dim): { non-fluent, real, default = 6.0 };
		GOAL(dim): { non-fluent, real, default = 10.0 };
		PENALTY: {non-fluent, real, default = 1000000.0 };
		
		// Interm		
		inMud:{interm-fluent, bool };
		mudCenter(dim):{interm-fluent, real };
 
		//State
		location(dim): {state-fluent, real, default = 0.0 };
				
		//Action
		move(dim): { action-fluent, real, default = 0.0 };
	};
	
	cpfs {
		inMud = forall_{?l:dim}[(location(?l) + move(?l) >= MINMUDBOUND(?l)) ^ (location(?l) + move(?l) <= MAXMUDBOUND(?l))];
		mudCenter(?l) = (MAXMUDBOUND(?l)-MINMUDBOUND(?l))/2.0;
		location'(?l) = if(inMud) then location(?l) + move(?l)*abs[location(?l)-mudCenter(?l)]/(MAXMUDBOUND(?l)-mudCenter(?l)) else location(?l) + move(?l);
	};
	
	reward = - sum_{?l: dim}[abs[GOAL(?l) - location(?l)]];
								
	state-action-constraints {
		forall_{?l:dim} move(?l)<=MAXACTIONBOUND(?l);
		forall_{?l:dim} move(?l)>=MINACTIONBOUND(?l);
		forall_{?l:dim} move(?l)+location(?l)<=MAXMAZEBOUND(?l);
		forall_{?l:dim} move(?l)+location(?l)>=MINMAZEBOUND(?l);
	};

}

non-fluents Navigation_non {
	domain = Navigation_Problem;
	objects{
		dim: {x,y};
	};
	non-fluents {
		MINMAZEBOUND(x) = -1.0;
	};
}

instance is1{
	domain = Navigation_Problem;
	non-fluents = Navigation_non;
	init-state{
		location(x) = 0.0;
		location(y) = 0.0;
	};
	max-nondef-actions = 2;
	horizon = 100;
	discount = 1.0;
}

